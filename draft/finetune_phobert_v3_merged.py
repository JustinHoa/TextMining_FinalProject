import os
import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
import torch.nn as nn

# ============================================================
# 1. Cáº¤U HÃŒNH
# ============================================================
os.environ["TOKENIZERS_PARALLELISM"] = "false"
seed = 42
torch.manual_seed(seed)
np.random.seed(seed)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# ============================================================
# 2. LOAD Dá»® LIá»†U
# ============================================================
dataset_id = "hihihohohehe/vifactcheck-normalized"
print(f"\nLoading dataset: {dataset_id}")
dataset = load_dataset(dataset_id)

# ============================================================
# 3. CHIáº¾N LÆ¯á»¢C Gá»˜P LABELS
# ============================================================
print("\n" + "="*60)
print("APPLYING LABEL MERGING STRATEGY")
print("="*60)

# Mapping gá»™p labels theo chiáº¿n lÆ°á»£c Ä‘á» xuáº¥t
LABEL_MERGE_MAPPING = {
    # NhÃ³m 1: Kinh táº¿ & Thá»‹ trÆ°á»ng
    "Báº¤T Äá»˜NG Sáº¢N": "KINH Táº¾",  # Gá»™p vÃ o KINH Táº¾
    
    # NhÃ³m 2: VÄƒn hÃ³a - Giáº£i trÃ­ - Äá»i sá»‘ng
    "DU Lá»ŠCH": "VÄ‚N HÃ“A",  # Du lá»‹ch thÆ°á»ng gáº¯n vá»›i vÄƒn hÃ³a
    "GIá»šI TRáºº": "XÃƒ Há»˜I Äá»œI Sá»NG",  # Lá»‘i sá»‘ng giá»›i tráº»
    
    # NhÃ³m 3: ChÃ­nh trá»‹ - XÃ£ há»™i - Thá»i sá»±
    "QUÃ‚N Sá»°": "THáº¾ GIá»šI",  # QuÃ¢n sá»± thÆ°á»ng lÃ  tin quá»‘c táº¿
    "THá»œI Sá»°": "XÃƒ Há»˜I Äá»œI Sá»NG",  # Thá»i sá»± tá»•ng há»£p -> XÃ£ há»™i
    
    # CÃ¡c labels giá»¯ nguyÃªn
    "CHÃNH TRá»Š": "CHÃNH TRá»Š",
    "GIÃO Dá»¤C": "GIÃO Dá»¤C",
    "GIáº¢I TRÃ": "GIáº¢I TRÃ",
    "KHOA Há»ŒC CÃ”NG NGHá»†": "KHOA Há»ŒC CÃ”NG NGHá»†",
    "KINH Táº¾": "KINH Táº¾",
    "PHÃP LUáº¬T": "PHÃP LUáº¬T",
    "Sá»¨C KHá»E": "Sá»¨C KHá»E",
    "THáº¾ GIá»šI": "THáº¾ GIá»šI",
    "THá»‚ THAO": "THá»‚ THAO",
    "VÄ‚N HÃ“A": "VÄ‚N HÃ“A",
    "XÃƒ Há»˜I Äá»œI Sá»NG": "XÃƒ Há»˜I Äá»œI Sá»NG",
}

print("\nLabel merging strategy:")
print("  Báº¤T Äá»˜NG Sáº¢N (17) â†’ KINH Táº¾")
print("  DU Lá»ŠCH (20) â†’ VÄ‚N HÃ“A")
print("  GIá»šI TRáºº (27) â†’ XÃƒ Há»˜I Äá»œI Sá»NG")
print("  QUÃ‚N Sá»° (15) â†’ THáº¾ GIá»šI")
print("  THá»œI Sá»° (115) â†’ XÃƒ Há»˜I Äá»œI Sá»NG")

def merge_labels(example):
    """Gá»™p labels theo mapping"""
    old_label = example["New Topic 1"]
    new_label = LABEL_MERGE_MAPPING.get(old_label, old_label)
    example["Merged Topic"] = new_label
    return example

print("\nApplying label merging...")
dataset = dataset.map(merge_labels)

# Táº¡o label mapping má»›i
unique_labels = sorted(list(set(dataset['train']['Merged Topic'])))
num_labels = len(unique_labels)
label2id = {label: idx for idx, label in enumerate(unique_labels)}
id2label = {idx: label for label, idx in label2id.items()}

print(f"\nâœ… After merging: {num_labels} classes")
print(f"Labels: {unique_labels}\n")

# ============================================================
# 4. THÃŠM Cá»˜T LABEL
# ============================================================
def add_label_column(example):
    example['label'] = label2id[example['Merged Topic']]
    return example

dataset = dataset.map(add_label_column)

# Kiá»ƒm tra distribution
from collections import Counter
train_label_dist = Counter(dataset['train']['label'])
print("Train label distribution:")
for label_id in sorted(train_label_dist.keys()):
    label_name = id2label[label_id]
    count = train_label_dist[label_id]
    print(f"  {label_id:2d}. {label_name:25s}: {count:4d} samples")

# ============================================================
# 5. CLASS WEIGHTS CHO IMBALANCED DATA
# ============================================================
print("\nğŸ”§ Computing class weights...")
train_labels = np.array(dataset['train']['label'])
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_labels),
    y=train_labels
)
class_weights_tensor = torch.FloatTensor(class_weights).to(device)
print(f"Class weights computed: {class_weights.round(2)}")

# ============================================================
# 6. CUSTOM TRAINER Vá»šI WEIGHTED LOSS + LABEL SMOOTHING
# ============================================================
class WeightedLabelSmoothingTrainer(Trainer):
    def __init__(self, label_smoothing=0.1, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.label_smoothing = label_smoothing
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        
        # Weighted Cross Entropy vá»›i Label Smoothing
        loss_fct = nn.CrossEntropyLoss(
            weight=class_weights_tensor,
            label_smoothing=self.label_smoothing
        )
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss

# ============================================================
# 7. TOKENIZER & PREPROCESSING
# ============================================================
LOCAL_MODEL_PATH = "./phobert-base-v2"
if os.path.exists(LOCAL_MODEL_PATH):
    model_ckpt = LOCAL_MODEL_PATH
    print(f"\nâœ… Loading LOCAL model: {model_ckpt}")
else:
    model_ckpt = "vinai/phobert-base-v2"
    print(f"\nâ¬‡ï¸ Loading HuggingFace model: {model_ckpt}")

tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

def preprocess_function(examples):
    return tokenizer(
        examples["Statement"], 
        truncation=True,
        max_length=256,
    )

print("Tokenizing dataset...")
columns_to_remove = [col for col in dataset['train'].column_names if col != 'label']
tokenized_datasets = dataset.map(
    preprocess_function, 
    batched=True,
    remove_columns=columns_to_remove
)

print(f"Tokenized dataset: {tokenized_datasets}")
print(f"Columns: {tokenized_datasets['train'].column_names}")

# ============================================================
# 8. METRICS
# ============================================================
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    acc = accuracy_score(labels, predictions)
    f1_weighted = f1_score(labels, predictions, average='weighted')
    f1_macro = f1_score(labels, predictions, average='macro')
    
    return {
        "accuracy": acc,
        "f1_weighted": f1_weighted,
        "f1_macro": f1_macro,
    }

# ============================================================
# 9. KHá»I Táº O MODEL
# ============================================================
print(f"\nLoading model: {model_ckpt}")
model = AutoModelForSequenceClassification.from_pretrained(
    model_ckpt, 
    num_labels=num_labels,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)

# ============================================================
# 10. TRAINING ARGUMENTS - Tá»I Æ¯U HÃ“A
# ============================================================
output_dir = f"./draft/finetuned-phobert-merged-{num_labels}classes"

training_args = TrainingArguments(
    output_dir=output_dir,
    
    # Learning rate schedule
    learning_rate=2e-5,  # TÄƒng lÃªn vÃ¬ Ã­t classes hÆ¡n
    warmup_ratio=0.15,  # Warmup 15% Ä‘á»ƒ á»•n Ä‘á»‹nh
    lr_scheduler_type="cosine",  # Cosine decay
    
    # Batch size
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,  # Effective batch = 32
    
    # Epochs
    num_train_epochs=20,  # TÄƒng lÃªn 20 epochs
    
    # Regularization
    weight_decay=0.01,
    max_grad_norm=1.0,
    
    # Evaluation & Saving
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="f1_weighted",
    greater_is_better=True,
    
    # Mixed Precision
    fp16=torch.cuda.is_available(),
    
    # Data loading
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
    
    # Logging
    logging_dir='./logs',
    logging_steps=50,
    logging_strategy="steps",
    
    # Other
    seed=seed,
    report_to="none",
    disable_tqdm=False,
)

print(f"\nğŸ“‹ Training Configuration:")
print(f"  - Number of classes: {num_labels}")
print(f"  - Learning rate: {training_args.learning_rate}")
print(f"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"  - Epochs: {training_args.num_train_epochs}")
print(f"  - Warmup ratio: {training_args.warmup_ratio}")
print(f"  - LR scheduler: {training_args.lr_scheduler_type}")
print(f"  - Label smoothing: 0.1")
print(f"  - Class weighting: Enabled")
print(f"  - FP16: {training_args.fp16}")
print(f"  - Output dir: {output_dir}\n")

# ============================================================
# 11. DATA COLLATOR
# ============================================================
data_collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    padding=True
)

# ============================================================
# 12. TRAINER
# ============================================================
trainer = WeightedLabelSmoothingTrainer(
    label_smoothing=0.1,  # Label smoothing = 0.1
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["dev"],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# ============================================================
# 13. TRAINING
# ============================================================
print("=" * 60)
print("ğŸš€ STARTING TRAINING WITH OPTIMIZATIONS")
print("=" * 60)
print("Optimizations applied:")
print("  âœ… Label merging (16 â†’ 11 classes)")
print("  âœ… Class weighting")
print("  âœ… Label smoothing (0.1)")
print("  âœ… Cosine LR schedule")
print("  âœ… Gradient accumulation")
print("=" * 60 + "\n")

trainer.train()

print("\n" + "=" * 60)
print("âœ… TRAINING COMPLETED!")
print("=" * 60)

# ============================================================
# 14. EVALUATION ON TEST SET
# ============================================================
print("\n" + "=" * 60)
print("ğŸ“Š EVALUATING ON TEST SET...")
print("=" * 60)

test_results = trainer.evaluate(tokenized_datasets["test"])
print(f"\n===== TEST METRICS =====")
print(f"Accuracy    : {test_results['eval_accuracy']:.4f}")
print(f"F1 Weighted : {test_results['eval_f1_weighted']:.4f}")
print(f"F1 Macro    : {test_results['eval_f1_macro']:.4f}")

# Detailed classification report
print("\n===== CLASSIFICATION REPORT =====")
predictions = trainer.predict(tokenized_datasets["test"])
y_pred = np.argmax(predictions.predictions, axis=1)
y_true = predictions.label_ids

print(classification_report(
    y_true, 
    y_pred, 
    target_names=[id2label[i] for i in range(num_labels)],
    digits=4,
    zero_division=0
))

# Per-class analysis
print("\n===== PER-CLASS ANALYSIS =====")
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, y_pred)
for i in range(num_labels):
    total = cm[i].sum()
    correct = cm[i, i]
    acc = correct / total if total > 0 else 0
    print(f"{id2label[i]:25s}: {correct:3d}/{total:3d} = {acc:.2%}")

# ============================================================
# 15. SAVE FINAL MODEL
# ============================================================
final_model_path = f"./draft/final_phobert_merged_{num_labels}classes"
trainer.save_model(final_model_path)
tokenizer.save_pretrained(final_model_path)

# LÆ°u label mapping
import json
mapping_path = f"{final_model_path}/label_mapping.json"
with open(mapping_path, 'w', encoding='utf-8') as f:
    json.dump({
        "label2id": label2id,
        "id2label": id2label,
        "merge_mapping": LABEL_MERGE_MAPPING
    }, f, ensure_ascii=False, indent=2)

print(f"\nâœ… Model saved to: {final_model_path}")
print(f"âœ… Label mapping saved to: {mapping_path}")

# ============================================================
# 16. TEST INFERENCE
# ============================================================
print("\n" + "=" * 60)
print("ğŸ§ª TESTING INFERENCE...")
print("=" * 60)

model = AutoModelForSequenceClassification.from_pretrained(final_model_path)
model.to(device)
model.eval()

def predict_topic(text, return_probs=False):
    inputs = tokenizer(
        text, 
        return_tensors="pt", 
        truncation=True, 
        max_length=256
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)
        pred_idx = torch.argmax(probs, dim=-1).item()
        confidence = probs[0, pred_idx].item()
    
    if return_probs:
        return id2label[pred_idx], confidence, probs[0].cpu().numpy()
    return id2label[pred_idx], confidence

# Test samples covering all merged classes
test_samples = [
    "Bá»™ GiÃ¡o dá»¥c cÃ´ng bá»‘ Ä‘iá»ƒm chuáº©n Ä‘áº¡i há»c nÄƒm 2024",
    "Äá»™i tuyá»ƒn Viá»‡t Nam giÃ nh chiáº¿n tháº¯ng 3-0 trÆ°á»›c ThÃ¡i Lan",
    "GiÃ¡ vÃ ng hÃ´m nay tÄƒng máº¡nh lÃªn má»©c cao nháº¥t trong nÄƒm",
    "Khoa há»c gia phÃ¡t hiá»‡n loáº¡i virus má»›i gÃ¢y nguy hiá»ƒm",
    "Chung cÆ° má»›i má»Ÿ bÃ¡n táº¡i quáº­n 2 vá»›i giÃ¡ Æ°u Ä‘Ã£i",  # BÄS â†’ KINH Táº¾
    "Lá»… há»™i hoa xuÃ¢n 2024 sáº½ Ä‘Æ°á»£c tá»• chá»©c táº¡i HÃ  Ná»™i",  # Du lá»‹ch â†’ VÄ‚N HÃ“A
    "Giá»›i tráº» SÃ i GÃ²n Ä‘á»• xÃ´ Ä‘i check-in quÃ¡n cÃ  phÃª má»›i",  # Giá»›i tráº» â†’ XHÄ
    "TÆ°á»›ng Má»¹ cáº£nh bÃ¡o vá» cÄƒng tháº³ng á»Ÿ Biá»ƒn ÄÃ´ng",  # QuÃ¢n sá»± â†’ THáº¾ GIá»šI
    "Tai náº¡n giao thÃ´ng nghiÃªm trá»ng trÃªn cao tá»‘c",  # Thá»i sá»± â†’ XHÄ
    "Quá»‘c há»™i thÃ´ng qua luáº­t má»›i vá» Ä‘áº¥t Ä‘ai",  # ChÃ­nh trá»‹
]

print("\nSample predictions:")
for i, text in enumerate(test_samples, 1):
    pred, conf = predict_topic(text)
    print(f"\n{i}. Text: {text}")
    print(f"   Predicted: {pred} (confidence: {conf:.2%})")

print("\n" + "=" * 60)
print("âœ… DONE!")
print("=" * 60)
print(f"\nFinal model saved at: {final_model_path}")
print(f"Number of classes: {num_labels}")
print(f"Test accuracy: {test_results['eval_accuracy']:.2%}")
print(f"Test F1 (weighted): {test_results['eval_f1_weighted']:.2%}")
